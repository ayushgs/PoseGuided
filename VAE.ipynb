{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#importing dependencies\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import _pickle as cpickle #to store model histories in a file\n",
    "import os\n",
    "import imageio\n",
    "from PIL import Image\n",
    "\n",
    "use_cuda = False\n",
    "device   = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "    device   = torch.device('cuda')\n",
    "print(use_cuda)    \n",
    "    \n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying parameters\n",
    "image_size = 28 #HACK to use MNIST architecture\n",
    "G_input_dim = 100\n",
    "G_output_dim = 3\n",
    "D_input_dim = 3\n",
    "D_output_dim = 1\n",
    "num_filters = [1024, 512, 256, 128]\n",
    "\n",
    "learning_rate = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "\n",
    "data_dir = './Train_data'\n",
    "save_dir = './DCGAN_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shreyashpandey/PoseGuided\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreyashpandey/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n"
     ]
    }
   ],
   "source": [
    "#loading data\n",
    "\n",
    "transform = transforms.Compose([transforms.Scale(image_size),\n",
    "                                transforms.Grayscale(), #Hack to make MNIST code work\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(214.0466981, 206.55220904, 203.99178198), \n",
    "                                                     std=(54.34939265, 55.62690195, 58.85794001))])\n",
    "                                \n",
    "\n",
    "df_data = dsets.ImageFolder(data_dir, transform = transform)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=df_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return F.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(data_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(data_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(data_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/32031 (0%)]\tLoss: 592.137512\n",
      "Train Epoch: 1 [1280/32031 (4%)]\tLoss: -63663.992188\n",
      "Train Epoch: 1 [2560/32031 (8%)]\tLoss: -81512.531250\n",
      "Train Epoch: 1 [3840/32031 (12%)]\tLoss: -82823.664062\n",
      "Train Epoch: 1 [5120/32031 (16%)]\tLoss: -84093.398438\n",
      "Train Epoch: 1 [6400/32031 (20%)]\tLoss: -84217.062500\n",
      "Train Epoch: 1 [7680/32031 (24%)]\tLoss: -84377.640625\n",
      "Train Epoch: 1 [8960/32031 (28%)]\tLoss: -84428.523438\n",
      "Train Epoch: 1 [10240/32031 (32%)]\tLoss: -84487.164062\n",
      "Train Epoch: 1 [11520/32031 (36%)]\tLoss: -84535.492188\n",
      "Train Epoch: 1 [12800/32031 (40%)]\tLoss: -84562.882812\n",
      "Train Epoch: 1 [14080/32031 (44%)]\tLoss: -84594.289062\n",
      "Train Epoch: 1 [15360/32031 (48%)]\tLoss: -84620.320312\n",
      "Train Epoch: 1 [16640/32031 (52%)]\tLoss: -84642.414062\n",
      "Train Epoch: 1 [17920/32031 (56%)]\tLoss: -84667.320312\n",
      "Train Epoch: 1 [19200/32031 (60%)]\tLoss: -84690.539062\n",
      "Train Epoch: 1 [20480/32031 (64%)]\tLoss: -84702.523438\n",
      "Train Epoch: 1 [21760/32031 (68%)]\tLoss: -84717.054688\n",
      "Train Epoch: 1 [23040/32031 (72%)]\tLoss: -84728.828125\n",
      "Train Epoch: 1 [24320/32031 (76%)]\tLoss: -84744.921875\n",
      "Train Epoch: 1 [25600/32031 (80%)]\tLoss: -84752.937500\n",
      "Train Epoch: 1 [26880/32031 (84%)]\tLoss: -84766.726562\n",
      "Train Epoch: 1 [28160/32031 (88%)]\tLoss: -84774.390625\n",
      "Train Epoch: 1 [29440/32031 (92%)]\tLoss: -84778.523438\n",
      "Train Epoch: 1 [30720/32031 (96%)]\tLoss: -84792.968750\n",
      "Train Epoch: 1 [7750/32031 (100%)]\tLoss: -84805.927419\n",
      "====> Epoch: 1 Average loss: -81537.1741\n",
      "====> Test set loss: -84805.7034\n",
      "Train Epoch: 2 [0/32031 (0%)]\tLoss: -84802.445312\n",
      "Train Epoch: 2 [1280/32031 (4%)]\tLoss: -84807.281250\n",
      "Train Epoch: 2 [2560/32031 (8%)]\tLoss: -84815.054688\n",
      "Train Epoch: 2 [3840/32031 (12%)]\tLoss: -84821.882812\n",
      "Train Epoch: 2 [5120/32031 (16%)]\tLoss: -84829.906250\n",
      "Train Epoch: 2 [6400/32031 (20%)]\tLoss: -84829.750000\n",
      "Train Epoch: 2 [7680/32031 (24%)]\tLoss: -84838.921875\n",
      "Train Epoch: 2 [8960/32031 (28%)]\tLoss: -84843.000000\n",
      "Train Epoch: 2 [10240/32031 (32%)]\tLoss: -84849.992188\n",
      "Train Epoch: 2 [11520/32031 (36%)]\tLoss: -84854.281250\n",
      "Train Epoch: 2 [12800/32031 (40%)]\tLoss: -84855.117188\n",
      "Train Epoch: 2 [14080/32031 (44%)]\tLoss: -84862.445312\n",
      "Train Epoch: 2 [15360/32031 (48%)]\tLoss: -84862.906250\n",
      "Train Epoch: 2 [16640/32031 (52%)]\tLoss: -84860.171875\n",
      "Train Epoch: 2 [17920/32031 (56%)]\tLoss: -84866.289062\n",
      "Train Epoch: 2 [19200/32031 (60%)]\tLoss: -84874.039062\n",
      "Train Epoch: 2 [20480/32031 (64%)]\tLoss: -84880.132812\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs+1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(64, 20).to(device)\n",
    "        sample = model.decode(sample).cpu()\n",
    "        save_image(sample.view(64, 1, 28, 28),\n",
    "                   'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
